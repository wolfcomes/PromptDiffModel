{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import ProcessedLigandPocketDataset,ProcessedLigandDataset\n",
    "from pathlib import Path\n",
    "import torch_geometric.transforms as T\n",
    "\n",
    "datadir = '../data/zinc_npz'\n",
    "data_transform = None\n",
    "\n",
    "train_dataset = ProcessedLigandDataset(Path(datadir, 'train.npz'), transform=data_transform)\n",
    "# test_dataset = ProcessedLigandDataset(Path(datadir, 'test.npz'), transform=data_transform)\n",
    "# val_dataset = ProcessedLigandDataset(Path(datadir, 'val.npz'), transform=data_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['names', 'pocket_coords', 'pocket_one_hot', 'pocket_mask', 'num_pocket_nodes', 'opt_lig_coords', 'opt_lig_one_hot', 'opt_lig_mask', 'num_opt_lig_atoms'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 3.7940, -0.7905,  2.9475], dtype=torch.float64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[4]['pocket_coords'].mean(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, num_workers=24, collate_fn=train_dataset.collate_fn, shuffle=False, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, num_workers=24, collate_fn=val_dataset.collate_fn, shuffle=False,pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, num_workers=24, collate_fn=test_dataset.collate_fn, shuffle=False,pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['names', 'pocket_coords', 'pocket_one_hot', 'pocket_mask', 'num_pocket_nodes', 'opt_lig_coords', 'opt_lig_one_hot', 'opt_lig_mask', 'num_opt_lig_atoms'])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_loader:\n",
    "    print(batch.keys())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1ady_A_rec_1ady_ham_lig_tt_docked_generated_5_docked_poses_minimized', '1b9t_A_rec_1vcj_iba_lig_tt_docked_0', '1acj_A_rec_1w6r_gnt_lig_tt_docked_5', '1acj_A_rec_1w6r_gnt_lig_tt_docked_generated_1_docked_poses_minimized', '1ads_A_rec_2inz_ohp_lig_tt_min_generated_1_docked_poses_minimized', '1acj_A_rec_1w6r_gnt_lig_tt_docked_generated_4_docked_poses_minimized', '1acj_A_rec_1w6r_gnt_lig_tt_docked_generated_5_docked_poses_minimized']\n"
     ]
    }
   ],
   "source": [
    "for i, batch in enumerate(train_loader):\n",
    "    if i == 1:  # 0-based index, so this is the second batch\n",
    "        print(batch['names'])\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from constants import dataset_params, FLOAT_TYPE, INT_TYPE\n",
    "def get_ligand_and_pocket(data,virtual_nodes):\n",
    "    \n",
    "    opt_ligand = {\n",
    "        'x': data['opt_lig_coords'].to('cuda', FLOAT_TYPE),\n",
    "        'one_hot': data['opt_lig_one_hot'].to('cuda', FLOAT_TYPE),\n",
    "        'size': data['num_opt_lig_atoms'].to('cuda', INT_TYPE),\n",
    "        'mask': data['opt_lig_mask'].to('cuda', INT_TYPE),\n",
    "    }\n",
    "    if virtual_nodes:\n",
    "        opt_ligand['num_virtual_atoms'] = data['num_virtual_atoms'].to('cuda', INT_TYPE)\n",
    "\n",
    "    pocket = {\n",
    "        'x': data['pocket_coords'].to('cuda', FLOAT_TYPE),\n",
    "        'one_hot': data['pocket_one_hot'].to('cuda', FLOAT_TYPE),\n",
    "        'size': data['num_pocket_nodes'].to('cuda', INT_TYPE),\n",
    "        'mask': data['pocket_mask'].to('cuda', INT_TYPE)\n",
    "    }\n",
    "\n",
    "\n",
    "    atom_num_2 = pocket['one_hot'].shape[0]\n",
    "    additional_tensor_2 = torch.tensor([[0, 1]]).repeat(atom_num_2, 1).to('cuda')\n",
    "    pocket['one_hot'] = torch.cat([pocket['one_hot'],additional_tensor_2],dim =1)\n",
    "\n",
    "    return pocket, opt_ligand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pocket, opt_ligand = get_ligand_and_pocket(batch, virtual_nodes=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sigma(gamma, target_tensor):\n",
    "        \"\"\"Computes sigma given gamma.\"\"\"\n",
    "        return inflate_batch_array(torch.sqrt(torch.sigmoid(gamma)),\n",
    "                                        target_tensor)\n",
    "    \n",
    "def inflate_batch_array(array, target):\n",
    "    \"\"\"\n",
    "    Inflates the batch array (array) with only a single axis\n",
    "    (i.e. shape = (batch_size,), or possibly more empty axes\n",
    "    (i.e. shape (batch_size, 1, ..., 1)) to match the target shape.\n",
    "    \"\"\"\n",
    "    target_shape = (array.size(0),) + (1,) * (len(target.size()) - 1)\n",
    "    return array.view(target_shape)\n",
    "\n",
    "class PositiveLinear(torch.nn.Module):\n",
    "    \"\"\"Linear layer with weights forced to be positive.\"\"\"\n",
    "\n",
    "    def __init__(self, in_features: int, out_features: int, bias: bool = True,\n",
    "                 weight_init_offset: int = -2):\n",
    "        super(PositiveLinear, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = torch.nn.Parameter(\n",
    "            torch.empty((out_features, in_features)))\n",
    "        if bias:\n",
    "            self.bias = torch.nn.Parameter(torch.empty(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.weight_init_offset = weight_init_offset\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self) -> None:\n",
    "        torch.nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "\n",
    "        with torch.no_grad():\n",
    "            self.weight.add_(self.weight_init_offset)\n",
    "\n",
    "        if self.bias is not None:\n",
    "            fan_in, _ = torch.nn.init._calculate_fan_in_and_fan_out(self.weight)\n",
    "            bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n",
    "            torch.nn.init.uniform_(self.bias, -bound, bound)\n",
    "\n",
    "    def forward(self, input):\n",
    "        positive_weight = F.softplus(self.weight)\n",
    "        return F.linear(input, positive_weight, self.bias)\n",
    "\n",
    "class GammaNetwork(torch.nn.Module):\n",
    "    \"\"\"The gamma network models a monotonic increasing function.\n",
    "    Construction as in the VDM paper.\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.l1 = PositiveLinear(1, 1)\n",
    "        self.l2 = PositiveLinear(1, 1024)\n",
    "        self.l3 = PositiveLinear(1024, 1)\n",
    "\n",
    "        self.gamma_0 = torch.nn.Parameter(torch.tensor([-5.]))\n",
    "        self.gamma_1 = torch.nn.Parameter(torch.tensor([10.]))\n",
    "        self.show_schedule()\n",
    "\n",
    "    def show_schedule(self, num_steps=50):\n",
    "        t = torch.linspace(0, 1, num_steps).view(num_steps, 1)\n",
    "        gamma = self.forward(t)\n",
    "        print('Gamma schedule:')\n",
    "        print(gamma.detach().cpu().numpy().reshape(num_steps))\n",
    "\n",
    "    def gamma_tilde(self, t):\n",
    "        l1_t = self.l1(t)\n",
    "        return l1_t + self.l3(torch.sigmoid(self.l2(l1_t)))\n",
    "\n",
    "    def forward(self, t):\n",
    "        zeros, ones = torch.zeros_like(t), torch.ones_like(t)\n",
    "        # Not super efficient.\n",
    "        gamma_tilde_0 = self.gamma_tilde(zeros)\n",
    "        gamma_tilde_1 = self.gamma_tilde(ones)\n",
    "        gamma_tilde_t = self.gamma_tilde(t)\n",
    "\n",
    "        # Normalize to [0, 1]\n",
    "        normalized_gamma = (gamma_tilde_t - gamma_tilde_0) / (\n",
    "                gamma_tilde_1 - gamma_tilde_0)\n",
    "\n",
    "        # Rescale to [gamma_0, gamma_1]\n",
    "        gamma = self.gamma_0 + (self.gamma_1 - self.gamma_0) * normalized_gamma\n",
    "\n",
    "        return gamma\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch_geometric.transforms as T\n",
    "from constants import dataset_params, FLOAT_TYPE, INT_TYPE\n",
    "import utils\n",
    "from torch.utils.data import DataLoader\n",
    "from equivariant_diffusion.dynamics import EGNNDynamics\n",
    "from equivariant_diffusion.conditional_model import ConditionalDDPM\n",
    "import math\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_info = dataset_params['crossdock_full']\n",
    "lig_type_encoder = dataset_info['atom_encoder']\n",
    "lig_type_decoder = dataset_info['atom_decoder']\n",
    "pocket_type_encoder = dataset_info['aa_encoder']\n",
    "pocket_type_decoder = dataset_info['aa_decoder']\n",
    "\n",
    "histogram_file = Path(datadir, 'size_distribution.npy')\n",
    "histogram = np.load(histogram_file).tolist()\n",
    "\n",
    "virtual_nodes = False\n",
    "data_transform = None\n",
    "max_num_nodes = 100\n",
    "\n",
    "if virtual_nodes:\n",
    "    # symbol = 'virtual'\n",
    "\n",
    "    symbol = 'Ne'  # visualize as Neon atoms\n",
    "    lig_type_encoder[symbol] = len(lig_type_encoder)\n",
    "    data_transform = utils.AppendVirtualNodes(\n",
    "        max_num_nodes, lig_type_encoder, symbol)\n",
    "    \n",
    "    virtual_atom = lig_type_encoder[symbol]\n",
    "    lig_type_decoder.append(symbol)\n",
    "\n",
    "\n",
    "    # Update dataset_info dictionary. This is necessary for using the\n",
    "    # visualization functions.\n",
    "    dataset_info['atom_encoder'] = lig_type_encoder\n",
    "    dataset_info['atom_decoder'] = lig_type_decoder\n",
    "\n",
    "atom_nf = len(lig_type_decoder)\n",
    "aa_nf = len(pocket_type_decoder)\n",
    "\n",
    "\n",
    "train_dataset = ProcessedLigandDataset(Path(datadir, 'train.npz'), transform=data_transform)\n",
    "test_dataset = ProcessedLigandDataset(Path(datadir, 'test.npz'), transform=data_transform)\n",
    "val_dataset = ProcessedLigandDataset(Path(datadir, 'val.npz'), transform=data_transform)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, num_workers=24, collate_fn=train_dataset.collate_fn, shuffle=False, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, num_workers=24, collate_fn=val_dataset.collate_fn, shuffle=False,pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, num_workers=24, collate_fn=test_dataset.collate_fn, shuffle=False,pin_memory=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "x_dims = 3\n",
    "joint_nf = 64\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "atom_nf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa_nf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "net_dynamics = EGNNDynamics(\n",
    "    atom_nf = atom_nf,\n",
    "    residue_nf = aa_nf,\n",
    "    n_dims = x_dims,\n",
    "    joint_nf = joint_nf,\n",
    "    device='cuda',\n",
    "    hidden_nf= 128,\n",
    "    act_fn=torch.nn.SiLU(),\n",
    "    n_layers= 5,\n",
    "    attention= True,\n",
    "    tanh=True,\n",
    "    norm_constant=1,\n",
    "    inv_sublayers=1,\n",
    "    sin_embedding=False,\n",
    "    normalization_factor=100,\n",
    "    aggregation_method= 'sum' ,\n",
    "    edge_cutoff_ligand=10,\n",
    "    edge_cutoff_pocket=4,\n",
    "    edge_cutoff_interaction=4,\n",
    "    update_pocket_coords= False,\n",
    "    reflection_equivariant=True,\n",
    "    edge_embedding_dim=8,\n",
    "    condition_vector = True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy of n_nodes: H[N] 2.806856393814087\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cddpm = ConditionalDDPM(\n",
    "            dynamics = net_dynamics,\n",
    "            atom_nf = atom_nf,\n",
    "            residue_nf = aa_nf,\n",
    "            n_dims = x_dims,\n",
    "            timesteps= 1000,\n",
    "            noise_schedule = 'polynomial_2',\n",
    "            noise_precision = 5.0e-4,\n",
    "            loss_type = 'l2',\n",
    "            norm_values = [1, 4],\n",
    "            size_histogram = histogram,\n",
    "            virtual_node_idx=lig_type_encoder[symbol] if virtual_nodes else None\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompts(data):\n",
    "    # 创建一个和data长度一致的张量，每个样本是[0, 0, 0]\n",
    "    prompts = torch.zeros((len(data['opt_lig_coords']), 3),device = 'cuda')  # 返回一个大小为 [len(data), 3] 的零张量\n",
    "    return prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Average NLL Loss: 0.5928559899330139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Validation Loss: 1.2838871479034424\n",
      "Test Loss: 1.1479606628417969\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# 假设你已经定义了模型、优化器和其他超参数\n",
    "optimizer = torch.optim.Adam(cddpm.parameters(), lr=0.001)  # 选择合适的学习率\n",
    "num_epochs = 1\n",
    "device = 'cuda'\n",
    "save_dir = '../checkpoints/zinc'  # 模型保存的文件夹路径\n",
    "loss_type = 'l2'\n",
    "\n",
    "# 创建保存目录（如果不存在）\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "# 训练循环\n",
    "for epoch in range(num_epochs):\n",
    "    # 设置模型为训练模式\n",
    "    cddpm.train()\n",
    "    cddpm.to(device)\n",
    "    total_loss = 0\n",
    "    pbar = tqdm(train_loader, desc=f'Epoch {epoch}', leave=False)\n",
    "\n",
    "    # 训练阶段\n",
    "    for batch in pbar:\n",
    "        # batch = {key: batch[key].to(device) for key in batch}\n",
    "\n",
    "        optimizer.zero_grad()  # 清空梯度\n",
    "\n",
    "        # 提取配体和口袋数据\n",
    "        pocket, opt_ligand = get_ligand_and_pocket(batch, virtual_nodes)\n",
    "        ref_ligand = None\n",
    "        prompt_labels = get_prompts(batch)\n",
    "\n",
    "        \n",
    "        # 计算损失，返回 (nll, info)\n",
    "        delta_log_px, error_t_lig, error_t_pocket, SNR_weight, \\\n",
    "        loss_0_x_ligand, loss_0_x_pocket, loss_0_h, neg_log_const_0, \\\n",
    "        kl_prior, t_int, xh_lig_hat, info = cddpm(ref_ligand, pocket, opt_ligand, prompt_labels, return_info=True)\n",
    "\n",
    "        if loss_type == 'l2':\n",
    "            actual_ligand_size = opt_ligand['size'] - opt_ligand['num_virtual_atoms'] if virtual_nodes else opt_ligand['size']\n",
    "\n",
    "            # normalize loss_t\n",
    "            denom_lig = x_dims * actual_ligand_size + \\\n",
    "                        cddpm.atom_nf * opt_ligand['size']\n",
    "            error_t_lig = error_t_lig / denom_lig\n",
    "            denom_pocket = (x_dims + cddpm.residue_nf) * pocket['size']\n",
    "            error_t_pocket = error_t_pocket / denom_pocket\n",
    "            loss_t = 0.5 * (error_t_lig + error_t_pocket)\n",
    "\n",
    "            # normalize loss_0\n",
    "            loss_0_x_ligand = loss_0_x_ligand / (x_dims * actual_ligand_size)\n",
    "            loss_0_x_pocket = loss_0_x_pocket / (x_dims * pocket['size'])\n",
    "            loss_0 = loss_0_x_ligand + loss_0_x_pocket + loss_0_h\n",
    "\n",
    "        nll = loss_t + loss_0 + kl_prior\n",
    "\n",
    "        # print(\"loss\", nll)\n",
    "        nll = nll.mean()\n",
    "\n",
    "        # 反向传播\n",
    "        nll.backward()\n",
    "\n",
    "        # 更新参数\n",
    "        optimizer.step()\n",
    "\n",
    "        # 累加损失\n",
    "        total_loss += nll.item()\n",
    "        pbar.set_postfix(nll_loss=nll.item())  # 更新进度条的后缀信息\n",
    "\n",
    "    print(f'Epoch {epoch}, Average NLL Loss: {total_loss / len(train_loader)}')\n",
    "\n",
    "    # 验证阶段\n",
    "    cddpm.eval()  # 设置模型为评估模式\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():  # 不需要计算梯度\n",
    "        for batch in val_loader:\n",
    "            # batch = {key: batch[key].to(device) for key in batch}\n",
    "            \n",
    "            pocket, opt_ligand= get_ligand_and_pocket(batch, virtual_nodes)\n",
    "            ref_ligand = None\n",
    "            prompt_labels = get_prompts(batch)\n",
    "\n",
    "            delta_log_px, error_t_lig, error_t_pocket, SNR_weight, \\\n",
    "            loss_0_x_ligand, loss_0_x_pocket, loss_0_h, neg_log_const_0, \\\n",
    "            kl_prior, t_int, xh_lig_hat, info = cddpm(ref_ligand, pocket,opt_ligand, prompt_labels, return_info=True)\n",
    "\n",
    "            if loss_type == 'l2':\n",
    "                actual_ligand_size = opt_ligand['size'] - opt_ligand['num_virtual_atoms'] if virtual_nodes else opt_ligand['size']\n",
    "\n",
    "                # normalize loss_t\n",
    "                denom_lig = x_dims * actual_ligand_size + \\\n",
    "                            cddpm.atom_nf * opt_ligand['size']\n",
    "                error_t_lig = error_t_lig / denom_lig\n",
    "                denom_pocket = (x_dims + cddpm.residue_nf) * pocket['size']\n",
    "                error_t_pocket = error_t_pocket / denom_pocket\n",
    "                loss_t = 0.5 * (error_t_lig + error_t_pocket)\n",
    "\n",
    "                # normalize loss_0\n",
    "                loss_0_x_ligand = loss_0_x_ligand / (x_dims * actual_ligand_size)\n",
    "                loss_0_x_pocket = loss_0_x_pocket / (x_dims * pocket['size'])\n",
    "                loss_0 = loss_0_x_ligand + loss_0_x_pocket + loss_0_h\n",
    "\n",
    "            nll = loss_t + loss_0 + kl_prior\n",
    "            nll = nll.mean()  # 将 nll 转换为标量\n",
    "            val_loss += nll.item()\n",
    "\n",
    "    print(f'Epoch {epoch}, Validation Loss: {val_loss / len(val_loader)}')\n",
    "\n",
    "    # 每个 epoch 后保存模型\n",
    "    torch.save(cddpm.state_dict(), os.path.join(save_dir, f'zinc_epoch_{epoch}.pth'))\n",
    "\n",
    "# 最终测试阶段\n",
    "cddpm.eval()  # 设置模型为评估模式\n",
    "test_loss = 0\n",
    "with torch.no_grad():  # 不需要计算梯度\n",
    "    for batch in test_loader:\n",
    "        # batch = {key: batch[key].to(device) for key in batch}\n",
    "\n",
    "        pocket, opt_ligand = get_ligand_and_pocket(batch, virtual_nodes)\n",
    "        ref_ligand = None\n",
    "        prompt_labels = get_prompts(batch)\n",
    "        \n",
    "        delta_log_px, error_t_lig, error_t_pocket, SNR_weight, \\\n",
    "        loss_0_x_ligand, loss_0_x_pocket, loss_0_h, neg_log_const_0, \\\n",
    "        kl_prior, t_int, xh_lig_hat, info = cddpm(ref_ligand, pocket,opt_ligand, prompt_labels, return_info=True)\n",
    "\n",
    "        if loss_type == 'l2':\n",
    "            actual_ligand_size = opt_ligand['size'] - opt_ligand['num_virtual_atoms'] if virtual_nodes else opt_ligand['size']\n",
    "\n",
    "            # normalize loss_t\n",
    "            denom_lig = x_dims * actual_ligand_size + \\\n",
    "                        cddpm.atom_nf * opt_ligand['size']\n",
    "            error_t_lig = error_t_lig / denom_lig\n",
    "            denom_pocket = (x_dims + cddpm.residue_nf) * pocket['size']\n",
    "            error_t_pocket = error_t_pocket / denom_pocket\n",
    "            loss_t = 0.5 * (error_t_lig + error_t_pocket)\n",
    "\n",
    "            # normalize loss_0\n",
    "            loss_0_x_ligand = loss_0_x_ligand / (x_dims * actual_ligand_size)\n",
    "            loss_0_x_pocket = loss_0_x_pocket / (x_dims * pocket['size'])\n",
    "            loss_0 = loss_0_x_ligand + loss_0_x_pocket + loss_0_h\n",
    "\n",
    "        nll = loss_t + loss_0 + kl_prior\n",
    "        nll = nll.mean()  # 将 nll 转换为标量\n",
    "        test_loss += nll.item()\n",
    "\n",
    "print(f'Test Loss: {test_loss / len(test_loader)}')\n",
    "\n",
    "# 最终保存模型\n",
    "torch.save(cddpm.state_dict(), os.path.join(save_dir, 'zinc_final.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mol",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
