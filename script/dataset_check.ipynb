{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import ProcessedLigandPocketDataset\n",
    "from pathlib import Path\n",
    "import torch_geometric.transforms as T\n",
    "\n",
    "datadir = '../data/docking_results/processed_crossdock_noH_full_temp'\n",
    "\n",
    "data_transform = None\n",
    "\n",
    "train_dataset = ProcessedLigandPocketDataset(Path(datadir, 'train.npz'), transform=data_transform)\n",
    "test_dataset = ProcessedLigandPocketDataset(Path(datadir, 'test.npz'), transform=data_transform)\n",
    "val_dataset = ProcessedLigandPocketDataset(Path(datadir, 'val.npz'), transform=data_transform)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['names', 'prompt_labels', 'ref_lig_coords', 'ref_lig_one_hot', 'ref_lig_bonds', 'ref_lig_mask', 'num_ref_lig_atoms', 'pocket_coords', 'pocket_one_hot', 'pocket_mask', 'num_pocket_nodes', 'opt_lig_coords', 'opt_lig_one_hot', 'opt_lig_bond', 'opt_lig_mask', 'num_opt_lig_atoms'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4639"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([11, 3])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[84]['prompt_labels'].size()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, num_workers=24, collate_fn=train_dataset.collate_fn, shuffle=False, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, num_workers=24, collate_fn=val_dataset.collate_fn, shuffle=False,pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, num_workers=24, collate_fn=test_dataset.collate_fn, shuffle=False,pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['names', 'prompt_labels', 'ref_lig_coords', 'ref_lig_one_hot', 'ref_lig_bonds', 'ref_lig_mask', 'num_ref_lig_atoms', 'pocket_coords', 'pocket_one_hot', 'pocket_mask', 'num_pocket_nodes', 'opt_lig_coords', 'opt_lig_one_hot', 'opt_lig_bond', 'opt_lig_mask', 'num_opt_lig_atoms'])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_loader:\n",
    "    print(batch.keys())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2hoz_A_rec_2hoz_pmp_lig_tt_min/2hoz_A_rec_2hoz_pmp_lig_tt_min_0_pocket10.pdb_2hoz_A_rec_2hoz_pmp_lig_tt_min/2hoz_A_rec_2hoz_pmp_lig_tt_min_0.sdf', '2hoz_A_rec_2hoz_pmp_lig_tt_min/2hoz_A_rec_2hoz_pmp_lig_tt_min_0_pocket10.pdb_2hoz_A_rec_2hoz_pmp_lig_tt_min/2hoz_A_rec_2hoz_pmp_lig_tt_min_0.sdf', '4y7g_A_rec_4y7f_48x_lig_tt_min/4y7g_A_rec_4y7f_48x_lig_tt_min_0_pocket10.pdb_4y7g_A_rec_4y7f_48x_lig_tt_min/4y7g_A_rec_4y7f_48x_lig_tt_min_0.sdf', '4y7g_A_rec_4y7f_48x_lig_tt_min/4y7g_A_rec_4y7f_48x_lig_tt_min_0_pocket10.pdb_4y7g_A_rec_4y7f_48x_lig_tt_min/4y7g_A_rec_4y7f_48x_lig_tt_min_0.sdf', '5ugp_A_rec_5vrw_ttp_lig_tt_docked/5ugp_A_rec_5vrw_ttp_lig_tt_docked_4_pocket10.pdb_5ugp_A_rec_5vrw_ttp_lig_tt_docked/5ugp_A_rec_5vrw_ttp_lig_tt_docked_4.sdf', '5ugp_A_rec_5vrw_ttp_lig_tt_docked/5ugp_A_rec_5vrw_ttp_lig_tt_docked_4_pocket10.pdb_5ugp_A_rec_5vrw_ttp_lig_tt_docked/5ugp_A_rec_5vrw_ttp_lig_tt_docked_4.sdf', '5ugp_A_rec_5vrw_ttp_lig_tt_docked/5ugp_A_rec_5vrw_ttp_lig_tt_docked_4_pocket10.pdb_5ugp_A_rec_5vrw_ttp_lig_tt_docked/5ugp_A_rec_5vrw_ttp_lig_tt_docked_4.sdf', '5ugp_A_rec_5vrw_ttp_lig_tt_docked/5ugp_A_rec_5vrw_ttp_lig_tt_docked_4_pocket10.pdb_5ugp_A_rec_5vrw_ttp_lig_tt_docked/5ugp_A_rec_5vrw_ttp_lig_tt_docked_4.sdf']\n"
     ]
    }
   ],
   "source": [
    "for i, batch in enumerate(train_loader):\n",
    "    if i == 1:  # 0-based index, so this is the second batch\n",
    "        print(batch['names'])\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from constants import dataset_params, FLOAT_TYPE, INT_TYPE\n",
    "def get_ligand_and_pocket(data,virtual_nodes):\n",
    "    ref_ligand = {\n",
    "        'x': data['ref_lig_coords'].to('cuda', FLOAT_TYPE),\n",
    "        'one_hot': data['ref_lig_one_hot'].to('cuda', FLOAT_TYPE),\n",
    "        'size': data['num_ref_lig_atoms'].to('cuda', INT_TYPE),\n",
    "        'mask': data['ref_lig_mask'].to('cuda', INT_TYPE),\n",
    "    }\n",
    "    if virtual_nodes:\n",
    "        ref_ligand['num_virtual_atoms'] = data['num_virtual_atoms'].to('cuda', INT_TYPE)\n",
    "    \n",
    "    opt_ligand = {\n",
    "        'x': data['opt_lig_coords'].to('cuda', FLOAT_TYPE),\n",
    "        'one_hot': data['opt_lig_one_hot'].to('cuda', FLOAT_TYPE),\n",
    "        'size': data['num_opt_lig_atoms'].to('cuda', INT_TYPE),\n",
    "        'mask': data['opt_lig_mask'].to('cuda', INT_TYPE),\n",
    "    }\n",
    "    if virtual_nodes:\n",
    "        opt_ligand['num_virtual_atoms'] = data['num_virtual_atoms'].to('cuda', INT_TYPE)\n",
    "\n",
    "    pocket = {\n",
    "        'x': data['pocket_coords'].to('cuda', FLOAT_TYPE),\n",
    "        'one_hot': data['pocket_one_hot'].to('cuda', FLOAT_TYPE),\n",
    "        'size': data['num_pocket_nodes'].to('cuda', INT_TYPE),\n",
    "        'mask': data['pocket_mask'].to('cuda', INT_TYPE)\n",
    "    }\n",
    "\n",
    "    atom_num_1 = ref_ligand['one_hot'].shape[0]\n",
    "    atom_num_2 = pocket['one_hot'].shape[0]\n",
    "    additional_tensor_1 = torch.tensor([[1, 0]]).repeat(atom_num_1, 1).to('cuda')\n",
    "    additional_tensor_2 = torch.tensor([[0, 1]]).repeat(atom_num_2, 1).to('cuda')\n",
    "    ref_ligand['one_hot'] = torch.cat((ref_ligand['one_hot'], additional_tensor_1), dim=1)\n",
    "    pocket['one_hot'] = torch.cat([pocket['one_hot'],additional_tensor_2],dim =1)\n",
    "\n",
    "    return ref_ligand, pocket, opt_ligand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_ligand, pocket, opt_ligand = get_ligand_and_pocket(batch, virtual_nodes=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([361, 361, 209, 209, 338, 338, 338, 338], device='cuda:0')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pocket['size']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([176, 11])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt_ligand['one_hot'].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pocket['mask'] = torch.cat([ref_ligand['mask'],pocket['mask']],dim =0)\n",
    "pocket['x'] = torch.cat([ref_ligand['x'],pocket['x']],dim =0)\n",
    "pocket['one_hot'] = torch.cat([ref_ligand['one_hot'],pocket['one_hot']],dim =0)\n",
    "pocket['size'] = ref_ligand['size'] + pocket['size']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "xh_lig = torch.cat([opt_ligand['x'], opt_ligand['one_hot']], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([176, 14])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xh_lig.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([15, 17, 13, 13, 30, 28, 30, 30], device='cuda:0')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt_ligand['size']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gamma schedule:\n",
      "[-5.         -4.6940923  -4.3876495  -4.0813856  -3.7749429  -3.4688568\n",
      " -3.1627707  -2.8566847  -2.5505986  -2.2445126  -1.9382484  -1.6318059\n",
      " -1.3255415  -1.0198119  -0.71336937 -0.4072833  -0.10101891  0.20488882\n",
      "  0.5109749   0.81706095  1.1235032   1.4294114   1.7358537   2.0415835\n",
      "  2.3480263   2.6537557   2.9603763   3.266284    3.5729046   3.8786345\n",
      "  4.184721    4.4908066   4.796715    5.1028004   5.408887    5.7149725\n",
      "  6.0212374   6.327324    6.6334095   6.9396734   7.2455816   7.551489\n",
      "  7.8577538   8.16384     8.470104    8.775833    9.082276    9.388006\n",
      "  9.694096   10.        ]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([176, 1])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sigma(gamma, target_tensor):\n",
    "        \"\"\"Computes sigma given gamma.\"\"\"\n",
    "        return inflate_batch_array(torch.sqrt(torch.sigmoid(gamma)),\n",
    "                                        target_tensor)\n",
    "    \n",
    "def inflate_batch_array(array, target):\n",
    "    \"\"\"\n",
    "    Inflates the batch array (array) with only a single axis\n",
    "    (i.e. shape = (batch_size,), or possibly more empty axes\n",
    "    (i.e. shape (batch_size, 1, ..., 1)) to match the target shape.\n",
    "    \"\"\"\n",
    "    target_shape = (array.size(0),) + (1,) * (len(target.size()) - 1)\n",
    "    return array.view(target_shape)\n",
    "class PositiveLinear(torch.nn.Module):\n",
    "    \"\"\"Linear layer with weights forced to be positive.\"\"\"\n",
    "\n",
    "    def __init__(self, in_features: int, out_features: int, bias: bool = True,\n",
    "                 weight_init_offset: int = -2):\n",
    "        super(PositiveLinear, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = torch.nn.Parameter(\n",
    "            torch.empty((out_features, in_features)))\n",
    "        if bias:\n",
    "            self.bias = torch.nn.Parameter(torch.empty(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.weight_init_offset = weight_init_offset\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self) -> None:\n",
    "        torch.nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "\n",
    "        with torch.no_grad():\n",
    "            self.weight.add_(self.weight_init_offset)\n",
    "\n",
    "        if self.bias is not None:\n",
    "            fan_in, _ = torch.nn.init._calculate_fan_in_and_fan_out(self.weight)\n",
    "            bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n",
    "            torch.nn.init.uniform_(self.bias, -bound, bound)\n",
    "\n",
    "    def forward(self, input):\n",
    "        positive_weight = F.softplus(self.weight)\n",
    "        return F.linear(input, positive_weight, self.bias)\n",
    "\n",
    "class GammaNetwork(torch.nn.Module):\n",
    "    \"\"\"The gamma network models a monotonic increasing function.\n",
    "    Construction as in the VDM paper.\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.l1 = PositiveLinear(1, 1)\n",
    "        self.l2 = PositiveLinear(1, 1024)\n",
    "        self.l3 = PositiveLinear(1024, 1)\n",
    "\n",
    "        self.gamma_0 = torch.nn.Parameter(torch.tensor([-5.]))\n",
    "        self.gamma_1 = torch.nn.Parameter(torch.tensor([10.]))\n",
    "        self.show_schedule()\n",
    "\n",
    "    def show_schedule(self, num_steps=50):\n",
    "        t = torch.linspace(0, 1, num_steps).view(num_steps, 1)\n",
    "        gamma = self.forward(t)\n",
    "        print('Gamma schedule:')\n",
    "        print(gamma.detach().cpu().numpy().reshape(num_steps))\n",
    "\n",
    "    def gamma_tilde(self, t):\n",
    "        l1_t = self.l1(t)\n",
    "        return l1_t + self.l3(torch.sigmoid(self.l2(l1_t)))\n",
    "\n",
    "    def forward(self, t):\n",
    "        zeros, ones = torch.zeros_like(t), torch.ones_like(t)\n",
    "        # Not super efficient.\n",
    "        gamma_tilde_0 = self.gamma_tilde(zeros)\n",
    "        gamma_tilde_1 = self.gamma_tilde(ones)\n",
    "        gamma_tilde_t = self.gamma_tilde(t)\n",
    "\n",
    "        # Normalize to [0, 1]\n",
    "        normalized_gamma = (gamma_tilde_t - gamma_tilde_0) / (\n",
    "                gamma_tilde_1 - gamma_tilde_0)\n",
    "\n",
    "        # Rescale to [gamma_0, gamma_1]\n",
    "        gamma = self.gamma_0 + (self.gamma_1 - self.gamma_0) * normalized_gamma\n",
    "\n",
    "        return gamma\n",
    "   \n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "gamma = GammaNetwork()\n",
    "gamma.to('cuda')\n",
    "lowest_t = 0\n",
    "time_step = 1000\n",
    "t_int = torch.randint(\n",
    "    lowest_t, time_step + 1, size=(ref_ligand['size'].size(0), 1),\n",
    "    device=ref_ligand['x'].device).float()\n",
    "s_int = t_int - 1  # previous timestep\n",
    "# Masks: important to compute log p(x | z0).\n",
    "t_is_zero = (t_int == 0).float()\n",
    "t_is_not_zero = 1 - t_is_zero\n",
    "# Normalize t to [0, 1]. Note that the negative\n",
    "# step of s will never be used, since then p(x | z0) is computed.\n",
    "s = s_int / time_step\n",
    "t = t_int / time_step\n",
    "# Compute gamma_s and gamma_t via the network.\n",
    "gamma_s = inflate_batch_array(gamma(s), opt_ligand['x'])\n",
    "gamma_t = inflate_batch_array(gamma(t), opt_ligand['x'])\n",
    "sigma_s = sigma(gamma_s, xh_lig)\n",
    "lig_mask = opt_ligand[\"mask\"]\n",
    "sigma_s[lig_mask].size()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "xh_lig = torch.cat([opt_ligand['x'], opt_ligand['one_hot']], dim=1)\n",
    "xh_pocket = torch.cat([pocket['x'], pocket['one_hot']], dim=1)\n",
    "zs_lig, zs_pocket = xh_lig, xh_pocket\n",
    "\n",
    "shift_net_out_ligand = torch.ones(234,14)\n",
    "shift_net_out_ligand = shift_net_out_ligand.to(zs_lig.device)\n",
    "\n",
    "ligand_mask = opt_ligand[\"mask\"]\n",
    "pocket_mask = pocket[\"mask\"]\n",
    "zt_lig = zs_lig + shift_net_out_ligand[ligand_mask]*zs_lig*(1/time_step) + sigma_s[ligand_mask]*(1.0/time_step)**(1/2)*torch.randn_like(zs_lig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigma(gamma, target_tensor):\n",
    "        \"\"\"Computes sigma given gamma.\"\"\"\n",
    "        return inflate_batch_array(torch.sqrt(torch.sigmoid(gamma)),\n",
    "                                        target_tensor)\n",
    "\n",
    "def alpha(gamma, target_tensor):\n",
    "    \"\"\"Computes alpha given gamma.\"\"\"\n",
    "    return inflate_batch_array(torch.sqrt(torch.sigmoid(-gamma)),\n",
    "                                    target_tensor)\n",
    "def sample_gaussian(size, device):\n",
    "        x = torch.randn(size, device=device)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_t = alpha(gamma_t, xh_lig)\n",
    "sigma_t = sigma(gamma_t, xh_lig)\n",
    "\n",
    "# Sample zt ~ Normal(alpha_t x, sigma_t)\n",
    "eps_lig = sample_gaussian(\n",
    "    size=(len(lig_mask), 14),\n",
    "    device=lig_mask.device)\n",
    "\n",
    "# Sample z_t given x, h for timestep t, from q(z_t | x, h)\n",
    "z_t_lig = alpha_t[lig_mask] * xh_lig + sigma_t[lig_mask] * eps_lig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from constants import dataset_params\n",
    "import utils\n",
    "from utils import AppendVirtualNodes\n",
    "\n",
    "dataset_info = dataset_params['crossdock_full']\n",
    "histogram_file = Path(datadir, 'size_distribution.npy')\n",
    "histogram = np.load(histogram_file).tolist()\n",
    "\n",
    "lig_type_encoder = dataset_info['atom_encoder']\n",
    "lig_type_decoder = dataset_info['atom_decoder']\n",
    "pocket_type_encoder = dataset_info['aa_encoder']\n",
    "pocket_type_decoder = dataset_info['aa_decoder']\n",
    "\n",
    "virtual_nodes = False\n",
    "data_transform = None\n",
    "max_num_nodes = len(histogram) - 1\n",
    "if virtual_nodes:\n",
    "    # symbol = 'virtual'\n",
    "\n",
    "    symbol = 'Ne'  # visualize as Neon atoms\n",
    "    lig_type_encoder[symbol] = len(lig_type_encoder)\n",
    "    data_transform = utils.AppendVirtualNodes(\n",
    "        max_num_nodes, lig_type_encoder, symbol)\n",
    "    \n",
    "    virtual_atom = lig_type_encoder[symbol]\n",
    "    lig_type_decoder.append(symbol)\n",
    "\n",
    "\n",
    "    # Update dataset_info dictionary. This is necessary for using the\n",
    "    # visualization functions.\n",
    "    dataset_info['atom_encoder'] = lig_type_encoder\n",
    "    dataset_info['atom_decoder'] = lig_type_decoder\n",
    "\n",
    "atom_nf = len(lig_type_decoder)\n",
    "aa_nf = len(pocket_type_decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_num_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C', 'N', 'O', 'S', 'B', 'Br', 'Cl', 'P', 'I', 'F', 'others']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lig_type_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ProcessedLigandPocketDataset(Path(datadir, 'train.npz'), transform=data_transform)\n",
    "test_dataset = ProcessedLigandPocketDataset(Path(datadir, 'test.npz'), transform=data_transform)\n",
    "val_dataset = ProcessedLigandPocketDataset(Path(datadir, 'val.npz'), transform=data_transform)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, num_workers=24, collate_fn=train_dataset.collate_fn, shuffle=False, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, num_workers=24, collate_fn=val_dataset.collate_fn, shuffle=False,pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, num_workers=24, collate_fn=test_dataset.collate_fn, shuffle=False,pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from equivariant_diffusion.dynamics import EGNNDynamics\n",
    "\n",
    "x_dims = 3\n",
    "joint_nf =4\n",
    "\n",
    "condition_vector = torch.tensor([0, 0, 1], dtype=torch.int, device='cuda')\n",
    "condition_vector = condition_vector.unsqueeze(0).expand(2, -1)\n",
    "\n",
    "\n",
    "net_dynamics = EGNNDynamics(\n",
    "    atom_nf = atom_nf,\n",
    "    residue_nf = aa_nf,\n",
    "    n_dims = x_dims,\n",
    "    joint_nf = joint_nf,\n",
    "    device='cuda',\n",
    "    hidden_nf=2,\n",
    "    act_fn=torch.nn.SiLU(),\n",
    "    n_layers= 2 ,\n",
    "    attention= True,\n",
    "    tanh=True,\n",
    "    norm_constant=1,\n",
    "    inv_sublayers=1,\n",
    "    sin_embedding=False,\n",
    "    normalization_factor=100,\n",
    "    aggregation_method= 'sum' ,\n",
    "    edge_cutoff_ligand=10,\n",
    "    edge_cutoff_pocket=4,\n",
    "    edge_cutoff_interaction=4,\n",
    "    update_pocket_coords= False,\n",
    "    reflection_equivariant=True,\n",
    "    edge_embedding_dim=8,\n",
    "    condition_vector = True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('../data/docking_results/processed_crossdock_noH_full_temp/size_distribution.npy')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "histogram_file = Path(datadir, 'size_distribution.npy')\n",
    "histogram_file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy of n_nodes: H[N] 8.910039901733398\n"
     ]
    }
   ],
   "source": [
    "from equivariant_diffusion.conditional_model import ConditionalDDPM\n",
    "\n",
    "\n",
    "x_dims = 3\n",
    "joint_nf =4\n",
    "\n",
    "\n",
    "cddpm = ConditionalDDPM(\n",
    "            dynamics = net_dynamics,\n",
    "            atom_nf = atom_nf,\n",
    "            residue_nf = aa_nf,\n",
    "            n_dims = x_dims,\n",
    "            timesteps= 100,\n",
    "            noise_schedule = 'polynomial_2',\n",
    "            noise_precision = 5.0e-4,\n",
    "            loss_type = 'l2',\n",
    "            norm_values = [1, 4],\n",
    "            size_histogram = histogram,\n",
    "            virtual_node_idx=lig_type_encoder[symbol] if virtual_nodes else None\n",
    "    )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamics = net_dynamics,\n",
    "atom_nf = atom_nf,\n",
    "residue_nf = aa_nf,\n",
    "n_dims = x_dims,\n",
    "timesteps= 100,\n",
    "noise_schedule = 'polynomial_2',\n",
    "noise_precision = 5.0e-4,\n",
    "loss_type = 'l2',\n",
    "norm_values = [1, 4],\n",
    "size_histogram = histogram,\n",
    "virtual_node_idx=lig_type_encoder[symbol] if virtual_nodes else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompts(data):\n",
    "    # 创建一个张量 [0, 0, 1]\n",
    "    prompts = torch.tensor(data['prompt_labels']).to('cuda', INT_TYPE)\n",
    "    \n",
    "    return prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_type = 'l2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/580 [00:00<?, ?it/s]/tmp/ipykernel_2736632/3916911146.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  prompts = torch.tensor(data['prompt_labels']).to('cuda', INT_TYPE)\n",
      "                                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Average NLL Loss: 0.5761266875369796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "import utils\n",
    "from utils import AppendVirtualNodes\n",
    "\n",
    "# 假设你已经定义了模型、优化器和其他超参数\n",
    "optimizer = torch.optim.Adam(cddpm.parameters(), lr=0.001)  # 选择合适的学习率\n",
    "num_epochs = 1\n",
    "device = 'cuda'\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    cddpm.train()  # 设置模型为训练模式\n",
    "    cddpm.to(device)\n",
    "    total_loss = 0\n",
    "    pbar = tqdm(train_loader, desc=f'Epoch {epoch}', leave=False)\n",
    "\n",
    "    for batch in pbar:\n",
    "        # 假设 batch 是一个列表，包含多个样本\n",
    "        # 将每个样本移动到设备并组合成字典\n",
    "        # batch={key:batch[key].cuda() for key in batch}\n",
    "\n",
    "        \n",
    "        optimizer.zero_grad()  # 清空梯度\n",
    "\n",
    "        ref_ligand, pocket, opt_ligand = get_ligand_and_pocket(batch,virtual_nodes)\n",
    "        prompt_labels = get_prompts(batch)\n",
    "        pocket['mask'] = torch.cat([ref_ligand['mask'],pocket['mask']],dim =0)\n",
    "        pocket['x'] = torch.cat([ref_ligand['x'],pocket['x']],dim =0)\n",
    "        pocket['one_hot'] = torch.cat([ref_ligand['one_hot'],pocket['one_hot']],dim =0)\n",
    "        pocket['size'] = ref_ligand['size'] + pocket['size']\n",
    "\n",
    "\n",
    "        loss_terms = cddpm(ref_ligand, pocket, opt_ligand, prompt_labels, return_info=False)\n",
    "\n",
    "        # delta_log_px, error_t_lig, error_t_pocket, SNR_weight, \\\n",
    "        # loss_0_x_ligand, loss_0_x_pocket, loss_0_h, neg_log_const_0, \\\n",
    "        # kl_prior, log_pN, t_int, xh_lig_hat, info = \\\n",
    "        #     cddpm(ref_ligand, pocket, opt_ligand, prompt_labels, return_info=True)\n",
    "        \n",
    "        delta_log_px, error_t_lig, error_t_pocket, SNR_weight, \\\n",
    "        loss_0_x_ligand, loss_0_x_pocket, loss_0_h, neg_log_const_0, \\\n",
    "        kl_prior, t_int, xh_lig_hat, info = \\\n",
    "            cddpm(ref_ligand, pocket, opt_ligand, prompt_labels, return_info=True)\n",
    "\n",
    "        if loss_type == 'l2':\n",
    "            actual_ligand_size = ref_ligand['size'] - ref_ligand['num_virtual_atoms'] if virtual_nodes else ref_ligand['size']\n",
    "\n",
    "            # normalize loss_t\n",
    "            denom_lig = x_dims * actual_ligand_size + \\\n",
    "                        cddpm.atom_nf * ref_ligand['size']\n",
    "            error_t_lig = error_t_lig / denom_lig\n",
    "            denom_pocket = (x_dims + cddpm.residue_nf) * pocket['size']\n",
    "            error_t_pocket = error_t_pocket / denom_pocket\n",
    "            loss_t = 0.5 * (error_t_lig + error_t_pocket)\n",
    "\n",
    "\n",
    "            # normalize loss_0\n",
    "            loss_0_x_ligand = loss_0_x_ligand / (x_dims * actual_ligand_size)\n",
    "            loss_0_x_pocket = loss_0_x_pocket / (x_dims * pocket['size'])\n",
    "            loss_0 = loss_0_x_ligand + loss_0_x_pocket + loss_0_h\n",
    "\n",
    "\n",
    "        nll = loss_t + loss_0 + kl_prior\n",
    "\n",
    "        # print(\"loss\", nll)\n",
    "        nll = nll.mean()\n",
    "\n",
    "        # 反向传播\n",
    "        nll.backward()\n",
    "\n",
    "        # 更新参数\n",
    "        optimizer.step()\n",
    "\n",
    "        # 累加损失\n",
    "        total_loss += nll.item()\n",
    "        pbar.set_postfix(nll_loss=nll.item())  # 更新进度条的后缀信息\n",
    "\n",
    "    print(f'Epoch {epoch}, Average NLL Loss: {total_loss / len(train_loader)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/580 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2736632/3916911146.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  prompts = torch.tensor(data['prompt_labels']).to('cuda', INT_TYPE)\n",
      "                                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Average NLL Loss: 0.5159810288199063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Validation Loss: 1.0647622297207515\n",
      "Test Loss: 1.06418603244755\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# 假设你已经定义了模型、优化器和其他超参数\n",
    "optimizer = torch.optim.Adam(cddpm.parameters(), lr=0.001)  # 选择合适的学习率\n",
    "num_epochs = 1\n",
    "device = 'cuda'\n",
    "save_dir = '../checkpoints/cddpm'  # 模型保存的文件夹路径\n",
    "\n",
    "# 创建保存目录（如果不存在）\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "# 训练循环\n",
    "for epoch in range(num_epochs):\n",
    "    # 设置模型为训练模式\n",
    "    cddpm.train()\n",
    "    cddpm.to(device)\n",
    "    total_loss = 0\n",
    "    pbar = tqdm(train_loader, desc=f'Epoch {epoch}', leave=False)\n",
    "\n",
    "    # 训练阶段\n",
    "    for batch in pbar:\n",
    "        # batch = {key: batch[key].to(device) for key in batch}\n",
    "\n",
    "        optimizer.zero_grad()  # 清空梯度\n",
    "\n",
    "        # 提取配体和口袋数据\n",
    "        ref_ligand, pocket, opt_ligand = get_ligand_and_pocket(batch,virtual_nodes)\n",
    "        prompt_labels = get_prompts(batch)\n",
    "\n",
    "        pocket['mask'] = torch.cat([ref_ligand['mask'],pocket['mask']],dim =0)\n",
    "        pocket['x'] = torch.cat([ref_ligand['x'],pocket['x']],dim =0)\n",
    "        pocket['one_hot'] = torch.cat([ref_ligand['one_hot'],pocket['one_hot']],dim =0)\n",
    "        pocket['size'] = ref_ligand['size'] + pocket['size']\n",
    "\n",
    "        # 计算损失，返回 (nll, info)\n",
    "        delta_log_px, error_t_lig, error_t_pocket, SNR_weight, \\\n",
    "        loss_0_x_ligand, loss_0_x_pocket, loss_0_h, neg_log_const_0, \\\n",
    "        kl_prior, t_int, xh_lig_hat, info = cddpm(ref_ligand, pocket, opt_ligand, prompt_labels, return_info=True)\n",
    "\n",
    "        if loss_type == 'l2':\n",
    "            actual_ligand_size = ref_ligand['size'] - ref_ligand['num_virtual_atoms'] if virtual_nodes else ref_ligand['size']\n",
    "\n",
    "            # normalize loss_t\n",
    "            denom_lig = x_dims * actual_ligand_size + \\\n",
    "                        cddpm.atom_nf * ref_ligand['size']\n",
    "            error_t_lig = error_t_lig / denom_lig\n",
    "            denom_pocket = (x_dims + cddpm.residue_nf) * pocket['size']\n",
    "            error_t_pocket = error_t_pocket / denom_pocket\n",
    "            loss_t = 0.5 * (error_t_lig + error_t_pocket)\n",
    "\n",
    "            # normalize loss_0\n",
    "            loss_0_x_ligand = loss_0_x_ligand / (x_dims * actual_ligand_size)\n",
    "            loss_0_x_pocket = loss_0_x_pocket / (x_dims * pocket['size'])\n",
    "            loss_0 = loss_0_x_ligand + loss_0_x_pocket + loss_0_h\n",
    "\n",
    "        nll = loss_t + loss_0 + kl_prior\n",
    "\n",
    "        # print(\"loss\", nll)\n",
    "        nll = nll.mean()\n",
    "\n",
    "        # 反向传播\n",
    "        nll.backward()\n",
    "\n",
    "        # 更新参数\n",
    "        optimizer.step()\n",
    "\n",
    "        # 累加损失\n",
    "        total_loss += nll.item()\n",
    "        pbar.set_postfix(nll_loss=nll.item())  # 更新进度条的后缀信息\n",
    "\n",
    "    print(f'Epoch {epoch}, Average NLL Loss: {total_loss / len(train_loader)}')\n",
    "\n",
    "    # 验证阶段\n",
    "    cddpm.eval()  # 设置模型为评估模式\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():  # 不需要计算梯度\n",
    "        for batch in val_loader:\n",
    "            # batch = {key: batch[key].to(device) for key in batch}\n",
    "            \n",
    "            ref_ligand, pocket, opt_ligand = get_ligand_and_pocket(batch, virtual_nodes)\n",
    "            prompt_labels = get_prompts(batch)\n",
    "\n",
    "            pocket['mask'] = torch.cat([ref_ligand['mask'],pocket['mask']],dim =0)\n",
    "            pocket['x'] = torch.cat([ref_ligand['x'],pocket['x']],dim =0)\n",
    "            pocket['one_hot'] = torch.cat([ref_ligand['one_hot'],pocket['one_hot']],dim =0)\n",
    "            pocket['size'] = ref_ligand['size'] + pocket['size']\n",
    "\n",
    "            delta_log_px, error_t_lig, error_t_pocket, SNR_weight, \\\n",
    "            loss_0_x_ligand, loss_0_x_pocket, loss_0_h, neg_log_const_0, \\\n",
    "            kl_prior, t_int, xh_lig_hat, info = cddpm(ref_ligand, pocket, opt_ligand, prompt_labels, return_info=True)\n",
    "\n",
    "            if loss_type == 'l2':\n",
    "                actual_ligand_size = ref_ligand['size'] - ref_ligand['num_virtual_atoms'] if virtual_nodes else ref_ligand['size']\n",
    "\n",
    "                # normalize loss_t\n",
    "                denom_lig = x_dims * actual_ligand_size + \\\n",
    "                            cddpm.atom_nf * ref_ligand['size']\n",
    "                error_t_lig = error_t_lig / denom_lig\n",
    "                denom_pocket = (x_dims + cddpm.residue_nf) * pocket['size']\n",
    "                error_t_pocket = error_t_pocket / denom_pocket\n",
    "                loss_t = 0.5 * (error_t_lig + error_t_pocket)\n",
    "\n",
    "                # normalize loss_0\n",
    "                loss_0_x_ligand = loss_0_x_ligand / (x_dims * actual_ligand_size)\n",
    "                loss_0_x_pocket = loss_0_x_pocket / (x_dims * pocket['size'])\n",
    "                loss_0 = loss_0_x_ligand + loss_0_x_pocket + loss_0_h\n",
    "\n",
    "            nll = loss_t + loss_0 + kl_prior\n",
    "            nll = nll.mean()  # 将 nll 转换为标量\n",
    "            val_loss += nll.item()\n",
    "\n",
    "    print(f'Epoch {epoch}, Validation Loss: {val_loss / len(val_loader)}')\n",
    "\n",
    "    # 每个 epoch 后保存模型\n",
    "    torch.save(cddpm.state_dict(), os.path.join(save_dir, f'cddpm_epoch_{epoch}.pth'))\n",
    "\n",
    "# 最终测试阶段\n",
    "cddpm.eval()  # 设置模型为评估模式\n",
    "test_loss = 0\n",
    "with torch.no_grad():  # 不需要计算梯度\n",
    "    for batch in test_loader:\n",
    "        # batch = {key: batch[key].to(device) for key in batch}\n",
    "\n",
    "        ref_ligand, pocket, opt_ligand = get_ligand_and_pocket(batch, virtual_nodes)\n",
    "        prompt_labels = get_prompts(batch)\n",
    "        \n",
    "        pocket['mask'] = torch.cat([ref_ligand['mask'],pocket['mask']],dim =0)\n",
    "        pocket['x'] = torch.cat([ref_ligand['x'],pocket['x']],dim =0)\n",
    "        pocket['one_hot'] = torch.cat([ref_ligand['one_hot'],pocket['one_hot']],dim =0)\n",
    "        pocket['size'] = ref_ligand['size'] + pocket['size']\n",
    "\n",
    "        delta_log_px, error_t_lig, error_t_pocket, SNR_weight, \\\n",
    "        loss_0_x_ligand, loss_0_x_pocket, loss_0_h, neg_log_const_0, \\\n",
    "        kl_prior, t_int, xh_lig_hat, info = cddpm(ref_ligand, pocket, opt_ligand, prompt_labels,return_info=True)\n",
    "\n",
    "        if loss_type == 'l2':\n",
    "            actual_ligand_size = ref_ligand['size'] - ref_ligand['num_virtual_atoms'] if virtual_nodes else ref_ligand['size']\n",
    "\n",
    "            # normalize loss_t\n",
    "            denom_lig = x_dims * actual_ligand_size + \\\n",
    "                        cddpm.atom_nf * ref_ligand['size']\n",
    "            error_t_lig = error_t_lig / denom_lig\n",
    "            denom_pocket = (x_dims + cddpm.residue_nf) * pocket['size']\n",
    "            error_t_pocket = error_t_pocket / denom_pocket\n",
    "            loss_t = 0.5 * (error_t_lig + error_t_pocket)\n",
    "\n",
    "            # normalize loss_0\n",
    "            loss_0_x_ligand = loss_0_x_ligand / (x_dims * actual_ligand_size)\n",
    "            loss_0_x_pocket = loss_0_x_pocket / (x_dims * pocket['size'])\n",
    "            loss_0 = loss_0_x_ligand + loss_0_x_pocket + loss_0_h\n",
    "\n",
    "        nll = loss_t + loss_0 + kl_prior\n",
    "        nll = nll.mean()  # 将 nll 转换为标量\n",
    "        test_loss += nll.item()\n",
    "\n",
    "print(f'Test Loss: {test_loss / len(test_loader)}')\n",
    "\n",
    "# 最终保存模型\n",
    "torch.save(cddpm.state_dict(), os.path.join(save_dir, 'cddpm_final2.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mol",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
